{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Project 2: Goals and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The goals of this assignment are:\n",
    "* To introduce you to some basic NLP tasks: tasks that operate over characters and words.\n",
    "* To introduce you to some of the ways NLP has been done over time, and how NLP illustrates the historical development of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here are the steps you should do to successfully complete this project:\n",
    "1. From moodle, accept the assignment.\n",
    "2. O;en and set up a code space (install a python kernal and select it).\n",
    "3. Complete the notebook and commit it to Github. Make sure to answer all questions, and to commit the notebook in a \"run\" state!\n",
    "4. Edit the README.md file. Provide your name, your class year and what you hope to get out of this course. Make sure to include the output from running second_nlp.py!\n",
    "5. Commit your code often. We will take the last commit before the deadline as your submission of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For extra credit:\n",
    "* Read the spaCy docs (https://spacy.io/usage/models). Figure out how to make spaCy work for another language. Add a mercury drop down menu (https://runmercury.com/docs/input-widgets/select/) for choosing a language. Include a screenshot of your modified web app running.\n",
    "* Read the spaCy docs for token-based matching (https://spacy.io/usage/rule-based-matching). Write a rule matcher for recognizing models of iPhone (iPhone, iphone, iPhone 6, etc). \n",
    "* Your other ideas are welcome! If you'd like to discuss one with Dr Stent, feel free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "This notebook uses content from:\n",
    "* The [Mercury tutorials](https://runmercury.com/tutorials/web-app-python-jupyter-notebook/)\n",
    "* The [spaCy docs](https://spacy.io/usage/linguistic-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Go to Moodle. Click on the link under the second lab session.\n",
    "\n",
    "Click to create/open a Codespace.\n",
    "\n",
    "Click on the notebook filename to open it (it ends with .ipynb).\n",
    "\n",
    "The first time you run it it will ask you to install python extensions and set the kernel type. We will walk through this in lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Be Sure To Save Your Work!!!**\n",
    "\n",
    "In Visual Studio, use the little blue dot on the left hand side to \"commit\" and \"sync\" your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Getting Started with NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will do steps 0-2 *every time* you do some NLP with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install and import spaCy\n",
    "\n",
    "First we have to **install** the spacy python package.\n",
    "\n",
    "(The ! tells Jupyter to execute this code as a regular Unix command, not python code. You will learn how to use Unix commands soon!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/python/3.10.8/lib/python3.10/site-packages (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (2.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a ML-driven NLP library so we also have to install a spaCy **model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must **import** (add) the [spacy NLP](https://spacy.io/) library.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```import spacy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up spaCy\n",
    "\n",
    "Now we must set up spaCy in python. We make a spaCy NLP **engine** and give it the **model**.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```nlp = spacy.load(\"en_core_web_sm\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a spaCy engine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a document and run spaCy on it\n",
    "\n",
    "Now we will create a document and run the NLP on it.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```text = \"I went for a drive recently. It was 23.2 miles round-trip. The road at the end was dirt and very narrow, so I had to make a sharp U-turn.\"```\n",
    "\n",
    "```doc = nlp(text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Tokenization\n",
    "\n",
    "When spaCy processes a document, the first thing it does is split it into **tokens**. \n",
    "\n",
    "A spaCy **token** is an example of a specialized data type in python called a **class**. We will learn more about how to create these specialized data types later in the semester. \n",
    "\n",
    "For now, just notice that:\n",
    "1. Each spaCy token contains text: to get the text, we say ```token.text```.\n",
    "2. spaCy does *not* make tokens for white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I # went # for # a # drive # recently # . # It # was # 23.2 # miles # round # - # trip # . # The # road # at # the # end # was # dirt # and # very # narrow # , # so # I # had # to # make # a # sharp # U # - # turn # .\n"
     ]
    }
   ],
   "source": [
    "token_texts = [token.text for token in doc]\n",
    "print(' # '.join(token_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is a Word? What is a Token?\n",
    "\n",
    "We are used to talking about language in terms of **words**. *The Concise Oxford Dictionary of Linguistics* defines a \"word\" as follows:\n",
    "\"\"\n",
    "\n",
    "As you can tell, this definition is not very precise! For computer software (even AI) we need *precise* definitions of things. \n",
    "\n",
    "All sorts of interesting questions come up when we try to talk about words:\n",
    "* Is a punctuation mark a word?\n",
    "* What about a number?\n",
    "* Sometimes hyphenated \"words\" are one word and sometimes more than one. Who decides?\n",
    "\n",
    "When a NLP software like spaCy tries to split text into words, we call that **tokenization**. We call the results **tokens**. Most **tokens** are words, but sometimes the tokenization will be wrong from a linguistic perspective.\n",
    "\n",
    "NLP researchers often spend a lot of time creating [guidelines](https://catalog.ldc.upenn.edu/docs/LDC2011T03/treebank/english-translation-treebank-guidelines.pdf) (see also [these guidelines](https://universaldependencies.org/u/overview/tokenization.html)) for tokenization, since tokens are the *most basic unit* of analysis for many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White Space Tokenization\n",
    "\n",
    "A super-simple tokenizer would just split text on \"white space\". Let's compare that with spaCy's tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I # went # for # a # drive # recently. # It # was # 23.2 # miles # round-trip. # The # road # at # the # end # was # dirt # and # very # narrow, # so # I # had # to # make # a # sharp # U-turn.\n"
     ]
    }
   ],
   "source": [
    "text = \"I went for a drive recently. It was 23.2 miles round-trip. The road at the end was dirt and very narrow, so I had to make a sharp U-turn.\"\n",
    "whitespace_tokens = text.split(' ')\n",
    "print(' # '.join(whitespace_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. *List two of the words from the document where you think the white space tokenizer tokenized correctly, and two where you think it didn't.*\n",
    "2. *List two of the words from the document where you think spaCy tokenized correctly, and two where you think it didn't.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: For now, we will assume that we *know* the language of the input text. Later in the semester, we will look at how to use NLP to *find out* the language of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Classes\n",
    "\n",
    "Quite often, we want to group words together by how they *behave*. For example, we have talked about:\n",
    "* nouns\n",
    "* verbs\n",
    "\n",
    "**Dictionary**\n",
    "\n",
    "We call these groups **word classes** and the labels of the groups **part of speech tags**. \n",
    "\n",
    "When a spaCy NLP engine runs on a document, it attaches a part of speech tag to each token. Actually, for our document it attaches two! \n",
    "* First, tags from a coarse-grained set of word classes described [here](https://universaldependencies.org/u/pos/index.html)\n",
    "* Second, for English, tags from a fine-grained set of word classes described [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "The second set of tags was developed as part of a famous project called the *Penn Treebank*; however, it was only for English. \n",
    "\n",
    "The first set of tags was developed as part of a famous project called *Universal Dependencies*, which aims to create consistently *annotated* natural language data sets (called **corpora**) for all natural languages. Most of spaCy's non-English models are trained on Universal Dependencies data.\n",
    "\n",
    "Let's look at the tags for each of our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON # VERB # ADP # DET # NOUN # ADV # PUNCT # PRON # AUX # NUM # NOUN # ADJ # PUNCT # NOUN # PUNCT # DET # NOUN # ADP # DET # NOUN # AUX # NOUN # CCONJ # ADV # ADJ # PUNCT # CCONJ # PRON # VERB # PART # VERB # DET # ADJ # NOUN # NOUN # NOUN # PUNCT\n"
     ]
    }
   ],
   "source": [
    "# These tags represent a small set of word classes\n",
    "token_coarse_tags = [token.pos_ for token in doc]\n",
    "print(' # '.join(token_coarse_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP # VBD # IN # DT # NN # RB # . # PRP # VBD # CD # NNS # JJ # HYPH # NN # . # DT # NN # IN # DT # NN # VBD # NN # CC # RB # JJ # , # CC # PRP # VBD # TO # VB # DT # JJ # NN # NN # NN # .\n"
     ]
    }
   ],
   "source": [
    "# These tags represent a larger set of word classes\n",
    "token_fine_tags = [token.tag_ for token in doc]\n",
    "print(' # '.join(token_fine_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags (especially the fine-grained ones) are not very descriptive! If you want more information about a tag, you can use spaCy's ```explain()``` function.\n",
    "\n",
    "In the cell below, type ```spacy.explain(VBD)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get more information about a part of speech tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "3. *Using the ```explain()``` function, look up \"NN\" and \"NNS\". What is the difference?*\n",
    "4. *Let's say I only wanted to print the text for the tokens that have part of speech tag NOUN. In the code cell below, write the code to do that. Run your code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n",
      "miles\n",
      "trip\n",
      "road\n",
      "end\n",
      "dirt\n",
      "U\n",
      "-\n",
      "turn\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. *Looking at the output of the code cell above, which of the tokens that spaCy says is a NOUN is not a noun?*\n",
    "6. *The part of speech tagger runs after the tokenizer. If the tokenizer is wrong, what does that mean for the part of speech tagger?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of Words\n",
    "\n",
    "We know that words have internal structure. For example, in your earliest English classes you learned about *prefixes* and *suffixes* like:\n",
    "\n",
    "* 'un' - reverses the meaning of a noun\n",
    "* 'ed' - makes a regular verb in the past tense\n",
    "\n",
    "The field of linguistics that looks at the structure of words is called **morphology**.\n",
    "\n",
    "**Dictionary**\n",
    "\n",
    "When it processes an English document, spaCy attaches two types of information to each token that are related to its structure:\n",
    "\n",
    "1. A **lemma** - a \"quick and dirty\" approximation of the root form of the word\n",
    "2. A **morphological analysis**  - a more complete analysis of the structure of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I # go # for # a # drive # recently # . # it # be # 23.2 # mile # round # - # trip # . # the # road # at # the # end # be # dirt # and # very # narrow # , # so # I # have # to # make # a # sharp # u # - # turn # .\n"
     ]
    }
   ],
   "source": [
    "# MAke them write it\n",
    "token_lemmas = [token.lemma_ for token in doc]\n",
    "print(' # '.join(token_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case=Nom|Number=Sing|Person=1|PronType=Prs # Tense=Past|VerbForm=Fin #  # Definite=Ind|PronType=Art # Number=Sing #  # PunctType=Peri # Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs # Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin # NumType=Card # Number=Plur # Degree=Pos # PunctType=Dash # Number=Sing # PunctType=Peri # Definite=Def|PronType=Art # Number=Sing #  # Definite=Def|PronType=Art # Number=Sing # Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin # Number=Sing # ConjType=Cmp #  # Degree=Pos # PunctType=Comm # ConjType=Cmp # Case=Nom|Number=Sing|Person=1|PronType=Prs # Tense=Past|VerbForm=Fin #  # VerbForm=Inf # Definite=Ind|PronType=Art # Degree=Pos # Number=Sing # Number=Sing # Number=Sing # PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "# Make them write it\n",
    "token_morphs = [str(token.morph) for token in doc]\n",
    "print(' # '.join(token_morphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also ispha isstop etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's In a Word? Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         I          I   PRON    PRP      X   True   True Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "      went         go   VERB    VBD   xxxx   True  False Tense=Past|VerbForm=Fin\n",
      "       for        for    ADP     IN    xxx   True   True \n",
      "         a          a    DET     DT      x   True   True Definite=Ind|PronType=Art\n",
      "     drive      drive   NOUN     NN   xxxx   True  False Number=Sing\n",
      "  recently   recently    ADV     RB   xxxx   True  False \n",
      "         .          .  PUNCT      .      .  False  False PunctType=Peri\n",
      "        It         it   PRON    PRP     Xx   True   True Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "       was         be    AUX    VBD    xxx   True   True Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "      23.2       23.2    NUM     CD   dd.d  False  False NumType=Card\n",
      "     miles       mile   NOUN    NNS   xxxx   True  False Number=Plur\n",
      "     round      round    ADJ     JJ   xxxx   True  False Degree=Pos\n",
      "         -          -  PUNCT   HYPH      -  False  False PunctType=Dash\n",
      "      trip       trip   NOUN     NN   xxxx   True  False Number=Sing\n",
      "         .          .  PUNCT      .      .  False  False PunctType=Peri\n",
      "       The        the    DET     DT    Xxx   True   True Definite=Def|PronType=Art\n",
      "      road       road   NOUN     NN   xxxx   True  False Number=Sing\n",
      "        at         at    ADP     IN     xx   True   True \n",
      "       the        the    DET     DT    xxx   True   True Definite=Def|PronType=Art\n",
      "       end        end   NOUN     NN    xxx   True  False Number=Sing\n",
      "       was         be    AUX    VBD    xxx   True   True Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "      dirt       dirt   NOUN     NN   xxxx   True  False Number=Sing\n",
      "       and        and  CCONJ     CC    xxx   True   True ConjType=Cmp\n",
      "      very       very    ADV     RB   xxxx   True   True \n",
      "    narrow     narrow    ADJ     JJ   xxxx   True  False Degree=Pos\n",
      "         ,          ,  PUNCT      ,      ,  False  False PunctType=Comm\n",
      "        so         so  CCONJ     CC     xx   True   True ConjType=Cmp\n",
      "         I          I   PRON    PRP      X   True   True Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "       had       have   VERB    VBD    xxx   True   True Tense=Past|VerbForm=Fin\n",
      "        to         to   PART     TO     xx   True   True \n",
      "      make       make   VERB     VB   xxxx   True   True VerbForm=Inf\n",
      "         a          a    DET     DT      x   True   True Definite=Ind|PronType=Art\n",
      "     sharp      sharp    ADJ     JJ   xxxx   True  False Degree=Pos\n",
      "         U          u   NOUN     NN      X   True  False Number=Sing\n",
      "         -          -   NOUN     NN      -  False  False Number=Sing\n",
      "      turn       turn   NOUN     NN   xxxx   True  False Number=Sing\n",
      "         .          .  PUNCT      .      .  False  False PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'%10s %10s %6s %6s %6s %6s %6s %s' % (token.text, token.lemma_, token.pos_, token.tag_, token.shape_, token.is_alpha, token.is_stop, token.morph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a mercury app\n",
    "\n",
    "and second_nlp.py\n",
    "\n",
    "guppy puppy monkey gorilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Finish the project\n",
    "\n",
    "* Make a file called 'first_nlp.py'. (To do this, click on the top left icon in Visual Studio, then on the Document+ icon.) Copy all the code from this section (Getting Started with NLP in Python) into that file.\n",
    "* Change the text in the line that starts with 'doc = ' to be ```doc = nlp(\"Natural language processing is a subfield of Artificial Intelligence!\")```.\n",
    "* Make sure to comment your code. (For now, aim for one comment per line of code.)\n",
    "* Save the file.\n",
    "* In the cell below, type: ```!python first_nlp.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "8. *How many tokens are in the sentence \"Natural language processing is a subfield of Artificial Intelligence!\"?*\n",
    "9. *Look at the [spacy quickstart](https://spacy.io/usage/spacy-101). What is one type of NLP other than tokenization that you would find useful?*\n",
    "10. *Write down one question you have after learning the basics of Jupyter notebooks, Github and spacy.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Project 2: Goals and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The goals of this assignment are:\n",
    "* To introduce you to some basic NLP tasks: tasks that operate over **words**.\n",
    "* To introduce you to some of the ways NLP has been done over time, and how NLP illustrates the historical development of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here are the steps you should do to successfully complete this project:\n",
    "1. From moodle, accept the assignment.\n",
    "2. Open and set up a code space (install a python kernal and select it).\n",
    "3. Complete the notebook and commit it to Github. Make sure to answer all questions, and to commit the notebook in a \"run\" state!\n",
    "4. Edit the README.md file. Provide your name, your class year and what you hope to get out of this course. Make sure to include the output from running second_nlp.py!\n",
    "5. Commit your code often. We will take the last commit before the deadline as your submission of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For extra credit:\n",
    "* Read the spaCy docs (https://spacy.io/usage/models). Figure out how to make spaCy work for another language. Add a mercury drop down menu (https://runmercury.com/docs/input-widgets/select/) for choosing a language. Include a screenshot of your modified web app running.\n",
    "* Read the spaCy docs for token-based matching (https://spacy.io/usage/rule-based-matching). Write a rule matcher for recognizing models of iPhone (iPhone, iphone, iPhone 6, etc). \n",
    "* ChatGPT vs spaCy! Pick a text, and run it through spaCy to get the tokens, lemmas, parts of speech and named entities. Then ask ChatGPT to give you the tokens, lemmas, parts of speech and named entities for the same text. Make a table listing the output from spaCy, the output from ChatGPT, and your analysis of each output.\n",
    "* Your other ideas are welcome! If you'd like to discuss one with Dr Stent, feel free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "This notebook uses content from:\n",
    "* The [spaCy docs](https://spacy.io/usage/linguistic-features)\n",
    "* The [Mercury tutorials](https://runmercury.com/tutorials/web-app-python-jupyter-notebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Go to Moodle. Click on the link under the second lab session.\n",
    "\n",
    "Click to create/open a Codespace.\n",
    "\n",
    "Click on the notebook filename to open it (it ends with .ipynb).\n",
    "\n",
    "The first time you run it it will ask you to install python extensions and set the kernel type. We will walk through this in lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Be Sure To Save Your Work!!!**\n",
    "\n",
    "In Visual Studio, use the little blue dot on the left hand side to \"commit\" and \"sync\" your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Getting Started with NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will do step 0 for *each new codespace* (every time you have to install a python kernel).\n",
    "\n",
    "You will do steps 1-2 *every time* you do some NLP with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install and import spaCy\n",
    "\n",
    "First we have to **install** the spacy python package.\n",
    "\n",
    "(The ! tells Jupyter to execute this code as a regular Unix command, not python code. You will learn how to use Unix commands soon!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a ML-driven NLP library so we also have to install a spaCy **model**.\n",
    "\n",
    "By the way, there are lots of spaCy models. To see a full list, click [here](https://spacy.io/usage/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must **import** (add) the [spacy NLP](https://spacy.io/) library.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```import spacy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must set up spaCy in python. We make a spaCy NLP **engine** and give it the **model**.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```nlp = spacy.load(\"en_core_web_sm\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a spaCy engine!\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a document and run spaCy on it\n",
    "\n",
    "Now we will create a document and run the NLP on it.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```text = \" Cathy and Amanda went for a drive to Portland, ME last June. It was 146.0 miles round-trip. We visited the Roux Institute and learned about AI-related graduate school programs.\"```\n",
    "\n",
    "```doc = nlp(text)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Things with Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Tokenization\n",
    "\n",
    "When spaCy processes a document, the first thing it does is split it into **tokens**. \n",
    "\n",
    "A spaCy **token** is an example of a specialized data type in python called a **class object**. We will learn more about how to create these specialized data types later in the semester. \n",
    "\n",
    "For now, just notice that:\n",
    "1. Each spaCy token contains (has an attribute for) text: to get the text, we say ```token.text```.\n",
    "2. spaCy does *not* make tokens for white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_texts = [token.text for token in doc]\n",
    "# Print the token_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a Word? What is a Token?\n",
    "\n",
    "We are used to talking about language in terms of **words**. *The Concise Oxford Dictionary of Linguistics* defines a \"word\" as follows:\n",
    "*\"Traditionally the smallest of the units that make up a sentence, and marked as such in writing....\"* (We will talk about sentences in a few weeks!)\n",
    "\n",
    "As you can tell, this definition is not very precise! For computer software (even AI) we need *precise* definitions of things. \n",
    "\n",
    "All sorts of interesting questions come up when we try to talk about words:\n",
    "* Is a punctuation mark a word?\n",
    "* What about a number?\n",
    "* Sometimes hyphenated \"words\" are one word and sometimes more than one. Who decides?\n",
    "\n",
    "When a NLP software like spaCy tries to split text into words, we call that **tokenization**. We call the results **tokens**. Most **tokens** are words, but sometimes the tokenization will be wrong from a linguistic perspective.\n",
    "\n",
    "NLP researchers often spend a lot of time creating [guidelines](https://catalog.ldc.upenn.edu/docs/LDC2011T03/treebank/english-translation-treebank-guidelines.pdf) (see also [these guidelines](https://universaldependencies.org/u/overview/tokenization.html)) for tokenization, since tokens are the *most basic unit* of analysis for many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Space Tokenization\n",
    "\n",
    "A super-simple tokenizer would just split text on \"white space\". Let's compare that with spaCy's tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokens = text.split(' ')\n",
    "# Print the whitespace_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. *List two of the words from the document where you think the white space tokenizer tokenized correctly, and two where you think it didn't.*\n",
    "2. *List two of the words from the document where you think spaCy tokenized correctly, and two where you think it didn't.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: For now, we will assume that we *know* the language of the input text. Later in the semester, we will look at how to use NLP to *find out* the language of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often, we want to group words together by how they *behave*. For example, we have talked about:\n",
    "* nouns\n",
    "* verbs\n",
    "\n",
    "We call these groups **word classes** and the labels of the groups **part of speech tags**. The Oxford Dictionary of Linguistics defines word class as follows: *\"Any class of word established by similarities in syntax or in grammar generally.\"*\n",
    "\n",
    "When a spaCy NLP engine runs on a document, it attaches a part of speech tag to each token. Actually, for our document it attaches two! \n",
    "* First, tags from a coarse-grained set of word classes described [here](https://universaldependencies.org/u/pos/index.html) and available via the token attribute `pos_`\n",
    "* Second, for English, tags from a fine-grained set of word classes described [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and available via the token attribute `tag_`\n",
    "\n",
    "The second set of tags was developed as part of a famous project called the *Penn Treebank*; however, it was only for English. \n",
    "\n",
    "The first set of tags was developed as part of a famous project called *Universal Dependencies*, which aims to create consistently *annotated* natural language data sets (called **corpora**) for all natural languages. Most of spaCy's non-English models are trained on Universal Dependencies data.\n",
    "\n",
    "Let's look at the tags for each of our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tags represent a small set of word classes\n",
    "token_coarse_tags = [token.pos_ for token in doc]\n",
    "# Print the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tags represent a larger set of word classes\n",
    "token_fine_tags = [token.tag_ for token in doc]\n",
    "# Print the tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags (especially the fine-grained ones) are not very descriptive! If you want more information about a tag, you can use spaCy's ```explain()``` function.\n",
    "\n",
    "In the cell below, type ```spacy.explain(VBD)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get more information about a part of speech tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "3. *Using the ```explain()``` function, look up the coarse and fine-grained part of speech tags and fill in this table:*\n",
    "\n",
    "| Word class | Coarse | Fine |\n",
    "| ---------- | ------ | ---- |\n",
    "| Noun       |        |      |\n",
    "| Verb       |        |      |\n",
    "| Adjective  |        |      |\n",
    "| Adverb     |        |      |\n",
    "\n",
    "4. *Compare \"NN\" and \"NNS\". What is the difference?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. *Which of the tokens that spaCy says is a NOUN is not a noun?*\n",
    "6. *The part of speech tagger runs after the tokenizer. If the tokenizer is wrong, what does that mean for the part of speech tagger?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Morphology\n",
    "\n",
    "We know that words have internal structure. For example, in your earliest English classes you learned about *prefixes* and *suffixes* like:\n",
    "\n",
    "* 'un' - reverses the meaning of a noun\n",
    "* 'ed' - makes a regular verb in the past tense\n",
    "\n",
    "The field of linguistics that looks at the structure of words is called **morphology**. The Oxford Dictionary of Linguistics defines morphology as: *\"The study of the grammatical structure of words and the categories realized by them.\"* It may be weird to think of a word as having (inside itself) a grammatical structure, but words definitely do. In English (and many other languages) this structure is realized through:\n",
    "\n",
    "* prefixes\n",
    "* suffixes\n",
    "* infixes\n",
    "\n",
    "When it processes an English document, spaCy attaches two types of information to each token that are related to its structure:\n",
    "\n",
    "1. A **lemma** - a \"quick and dirty\" approximation of the root form of the word, available via the token attribute `lemma_`\n",
    "2. A **morphological analysis**  - a more complete analysis of the structure of the word, available via the token attribute `morph`\n",
    "\n",
    "In the code cell below, print the lemmas for the tokens in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lemmas = [token.lemma_ for token in doc]\n",
    "# Print the lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the morphological analyses for the tokens in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_morphs = [str(token.morph) for token in doc]\n",
    "# Print the morphological analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other token attributes spaCy provides that are related to morphology. A full description is [here](https://spacy.io/api/token/). \n",
    "\n",
    "Questions:\n",
    "\n",
    "7. *What is one other type of token attribute you would like to use?*\n",
    "8. *What is one type of token attribute that we can just get via python string methods?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tokens (or multi-token units) are special because they *name* things. For example:\n",
    "\n",
    "* Names of people (like *Cathy Fan*)\n",
    "* Names of organizations (like *Colby College*)\n",
    "* Names of locations (like *Waterville*)\n",
    "\n",
    "spaCy attaches information about named entities to the document. In the code cell below, print the named entities for this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_texts = [ent.text for ent in doc.ents]\n",
    "entity_types = [ent.label_ for ent in doc.ents]\n",
    "# Print the entity_texts\n",
    "\n",
    "# Print the entity_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's In a Word? Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will practice our pretty printing, using the token attributes. We want to print a table, in python, with columns like this:\n",
    "\n",
    "| Token | Lemma | Coarse-grained Part of speech | Fine-grained Part of speech | Token shape | Token is punctuation? | Token morphology |\n",
    "| ----- | ----- | ----------------------------- | --------------------------- | ----------- | --------------------- | ---------------- |\n",
    "|       |       |                               |                             |             |                       |                  |\n",
    "\n",
    "I will give you the structure, which is a **for loop** (we will learn about this coming week!) and you write the print statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    # You pretty print the token attributes in order using the table above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a table of the entities, text and label for each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc:\n",
    "    # Pretty print the entity texts and labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our first web app! This app uses mercury, another python package. In order to use the web app, we have to install mercury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mercury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go to your **Terminal** and type `mercury run`. The web app will open in another tab. The file containing the web app is `project2_webapp.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Finish the project\n",
    "\n",
    "* Complete the file `second_nlp.py` following the comments in the file. Refer back to week1-2 as necessary.\n",
    "* In the cell below, type: ```!python second_nlp.py```. Remember you will enter your input in the pop-down text box from the top of the browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "9. *How many tokens are in the sentence \"Natural language processing is a subfield of Artificial Intelligence!\"?*\n",
    "10. *Look at the [spacy quickstart](https://spacy.io/usage/spacy-101). What is one type of NLP other than tokenization that you would find useful?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
